\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx, array, tabularx, multicol, wasysym, hyperref, scalerel, stackengine}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\urlstyle{same}
\usepackage[thmmarks,thref]{ntheorem}

\stackMath
\newcommand\reallywidehat[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}%
    {\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
  }{\textheight}% 
}{0.5ex}}%
\stackon[1pt]{#1}{\tmpbox}%
}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}[section]

\theoremstyle{definition}
\newtheorem*{example}{Example}[section]


\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{.}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{solution}{Solution}

\hypersetup{colorlinks=true,urlcolor=black}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}


\begin{document}
\SweaveOpts{concordance=TRUE}
\lhead{\today}
\chead{STAT 101 - Chapter Seven and Eight}
\rhead{Page \thepage\ of \pageref{LastPage}}

\section*{Chapter 7 and 8: Introduction to Linear Regression Part Two}

\subsection*{Review}

The goal of linear regression is to find a straight line that best represents the relationship between two quantitative variables. 

To quantify this relationship we take a response variable $y$ and an explanatory variable $x$, yielding the following equation:

\vskip 1in

where $\hat{y}$ is the predicted value of y. Recall that we can compute the \underline{\hspace{5cm}} \underline{\hspace{9cm}} using the following formulas.

$b_{o}: \hskip 3in  b_{1}:$

\subsection*{Introduction to Prediction}

If there is a linear relationship between two quantitative variables, we can use regression to make predictions.

\textbf{Prediction:} For some value of the explanatory variable, $x'$, that is of interest. We can plug in $x'$ into the regression line to predict the response variable:

\[
\hat{y} = b_{o} + b_{1}*x'
\]

\begin{example}
Suppose we are interested in studying corn yields( in hecters) and fertilizer(in pounds) amounts. After some intense research we find that the yield of corn can be characterized with the following linear regression:

\[
\reallywidehat{Yield} = 4.32 + 1.05*fertilizer
\]

Calculate the predicted yield when we apply 5 pounds of fertilizer to our field of corn. \vskip 1in
\end{example}

\newpage
\subsubsection*{Caution: Extrapolation}

It is generally not recommended to make predictions far outside the range of the explanatory variable. This is known as \underline{\hspace{9cm}}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width = 3in]{../images/extrapolation-example.png}
\end{figure}

For each of the following situations, determine whether each situations would be extrapolation.

\begin{enumerate}
  \item Predicting boiling point of water at a high altitude, using regression created from data at sea level. \vskip 1in
  \item Predicting test scores for middle schoolers at one school when the regression was created from data at a different school. \vskip 1in
  \item Predicting video game sales from data collected the year previous. \vskip 1in
  \item Predicting Usain Bolt's mile time from data collected from middle schooler mile run times.
\end{enumerate}
\newpage 
Let's return to our fire damage example. Recall that the relationship between amount of damage in thousands of dollars and the distance from a fire station in miles is characterized by the following regression equation:

\[
\reallywidehat{Damage} = 10.31 + 4.91*Distance
\]
\begin{figure}[ht!]
  \centering
  \includegraphics[width = 3in]{../images/fire-damage-linear-example.png}
\end{figure}

Calculate the predicted damage for each situation and determine whether there is extrapolation.

\begin{itemize}
  \item Distance of 5.5 miles \vskip 1in 
  \item Distance of 1.2 miles \vskip 1in
  \item Distance of 15 miles \vskip 1in
  \item Distance of 3 miles \vskip 1in
\end{itemize}
\newpage

\subsection*{Residuals}

The \underline{\hspace{6cm}} is the difference between the observed value and predicted value of the response variable. The residual value tells us how inaccurate our model's prediction is at that particular point.

\textbf{Notation:} e

The formula for calculating residuals for a particular observation $(x_{i}, y_{i})$ 
\[ e = y_{i} - \hat{y} \]

where $\hat{y} = b_o + b_1*x_1$ (the linear regression line evaluate at $x_1$)

On a scatterplot, a residual is the \underline{\hspace{10.5cm}}.
\begin{itemize}
  \item ~\\\\
  \item ~\\\\
\end{itemize}

\begin{example}
Once again let's look at our fire damage dataset. Compute the residual for each situation. 

\[
\reallywidehat{Damage} = 10.31 + 4.91*Distance
\]

\begin{itemize}
  \item A house that is 2.1 miles from the fire station experienced \$24,000 in damages. \vskip 2in
  \item A house that is 5.5 miles from the fire station experienced \$36,000 in damages.  \vskip 2in
\end{itemize}
\end{example}
\newpage

What does least squares estimate mean? Let's illustrate by drawing a picture. \vskip 5in
Another thing we can do with residuals is to compute their standard deviation. We can compute the standard deviation of the residuals the same way we calculate the standard deviation of other data. 

\textbf{Notation:} $s_e$ = standard deviation of the residuals

\textbf{Interpretation:} $s_e$ is a measure of the variation of the points around the regression line.
\begin{itemize} 
  \item large $s_e \implies$ lots of variation around the line
  \item small $s_e \implies$ little variation around the line
\end{itemize}


\end{document}